{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUN3Xpa95iwU"
   },
   "source": [
    "# TASK 6 - Fine Tuning BERT\n",
    "\n",
    "In this task we are asked to fine tune a BERT model for a different classification task. The BERT encoder stays the same while the BERT Classifier has to be manually added based on the task in hand. We were asked to follow [This](https://www.tensorflow.org/official_models/fine_tuning_bert) example. In the tensorflow example they have used MRPC dataset which has a pair of sentences and the classifier determines whether the sentence pairs are equivalent or not. We went on to train a classifier for cola dataset, where the network classifies if the given english sentence is grammatically valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "lrSJqU96cRGo",
    "outputId": "fff43456-efbf-43b2-f5cd-b6f3885a5478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 341.4MB 23kB/s \n",
      "\u001b[K     |████████████████████████████████| 9.2MB 54.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 460kB 56.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 983kB 5.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1MB 16.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 30.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 358kB 30.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 36.6MB 82kB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-nightly\n",
    "!pip install -q tf-models-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl85hTyP8fs1"
   },
   "source": [
    "### The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxGlRH4TLY1x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks\n",
    "\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIkiKnoG8ltP"
   },
   "source": [
    "### Links to all bert weights and training checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-u00DMqCLff2"
   },
   "outputs": [],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)\n",
    "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "360oX2yv9ysg"
   },
   "source": [
    "### loading the glue/cola dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "2bAHHqNFMSaG",
    "outputId": "52a01098-4a3f-41b2-906c-e75ae01060a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue/cola/1.0.0 (download: 368.14 KiB, generated: Unknown size, total: 368.14 KiB) to /root/tensorflow_datasets/glue/cola/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete7URXXU/glue-train.tfrecord\n",
      "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete7URXXU/glue-validation.tfrecord\n",
      "Shuffling and writing examples to /root/tensorflow_datasets/glue/cola/1.0.0.incomplete7URXXU/glue-test.tfrecord\n",
      "\u001b[1mDataset glue downloaded and prepared to /root/tensorflow_datasets/glue/cola/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sst2, info = tfds.load('glue/cola', with_info=True,\n",
    "                       batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "fDoJuU_MNK0j",
    "outputId": "50f2212e-3b4c-4dda-b891-5f259e975343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx      : 3393\n",
      "label    : 1\n",
      "sentence : b'They rowed the canals of Venice.'\n"
     ]
    }
   ],
   "source": [
    "sst2_train = sst2[\"train\"]\n",
    "for key,value in sst2_train.items():\n",
    "  print(f\"{key:9s}: {value[1000].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eayg_Ggi-Fxq"
   },
   "source": [
    "### calling the tokenizer\n",
    "\n",
    "the tokenizer offered by bert is used here. Since the bert encoding is done with this tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FKrFrqBdPE2x",
    "outputId": "9d72fe3f-9470-4690-a140-c1fcc285258f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "     do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-c8oJAP_idn"
   },
   "source": [
    "### Pre processor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_MVYt1pV0Ch"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(s):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "def preprocessing(sst2_dict,tokenizer):\n",
    "  count = len(sst2_dict[\"sentence\"])\n",
    "  sentence = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in sst2_dict[\"sentence\"]])\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n",
    "  input_word_ids = tf.concat([cls, sentence], axis=-1)\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_s = tf.zeros_like(sentence)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_s], axis=-1).to_tensor()\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids\n",
    "      }\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LopYgyWJ_ogW"
   },
   "source": [
    "### preprocessing the train, val, Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9GaMOv7YEP4"
   },
   "outputs": [],
   "source": [
    "glue_train = preprocessing(sst2['train'], tokenizer)\n",
    "glue_train_labels = sst2['train']['label']\n",
    "\n",
    "glue_validation = preprocessing(sst2['validation'], tokenizer)\n",
    "glue_validation_labels = sst2['validation']['label']\n",
    "\n",
    "glue_test = preprocessing(sst2['test'], tokenizer)\n",
    "glue_test_labels  = sst2['test']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kt80GAyZ_v7q"
   },
   "source": [
    "### Retriving the models , weights and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A-nWeweYi1F"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
    "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "_ , bert_encoder = bert.bert_models.classifier_model(\n",
    "    bert_config, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VluNB0kqBC4k"
   },
   "source": [
    "### Restoring the encoder checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qFTtlR9VbZUi",
    "outputId": "3c9b76de-9b65-440b-d503-7150f0eeca0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7730bb5b00>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
    "checkpoint.restore(\n",
    "    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQjVNPJV2M9B"
   },
   "outputs": [],
   "source": [
    "transformer_config = config_dict.copy()\n",
    "\n",
    "# You need to rename a few fields to make this work:\n",
    "transformer_config['attention_dropout_rate'] = transformer_config.pop('attention_probs_dropout_prob')\n",
    "transformer_config['activation'] = tf_utils.get_activation(transformer_config.pop('hidden_act'))\n",
    "transformer_config['dropout_rate'] = transformer_config.pop('hidden_dropout_prob')\n",
    "transformer_config['initializer'] = tf.keras.initializers.TruncatedNormal(\n",
    "          stddev=transformer_config.pop('initializer_range'))\n",
    "transformer_config['max_sequence_length'] = transformer_config.pop('max_position_embeddings')\n",
    "transformer_config['num_layers'] = transformer_config.pop('num_hidden_layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uknRGSKyBNDw"
   },
   "source": [
    "### Testing the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "id": "fFhR4YLm2ZuF",
    "outputId": "99e01262-7a3c-439c-944c-a18aa9f2b76f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(20, 1, 768), dtype=float32, numpy=\n",
      "array([[[ 2.7265620e-01,  2.3244005e-01,  2.9404190e-01, ...,\n",
      "         -2.0707411e-01,  4.6615210e-01, -5.0804846e-02]],\n",
      "\n",
      "       [[ 2.4580577e-01, -6.4193029e-03, -2.2461516e-01, ...,\n",
      "         -3.5705957e-01,  1.8671927e-01, -1.7014639e-01]],\n",
      "\n",
      "       [[ 6.0351956e-01, -5.4982886e-02,  5.9810340e-01, ...,\n",
      "          7.2259456e-05,  5.3612846e-01,  4.4433987e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.1136035e-01,  3.6061943e-01,  1.2609263e-01, ...,\n",
      "         -4.2029239e-02,  6.5500802e-01,  1.3683848e-01]],\n",
      "\n",
      "       [[ 4.1491485e-01, -4.2447838e-01,  3.3122575e-01, ...,\n",
      "          4.1855934e-01,  2.8256467e-01,  4.2693657e-01]],\n",
      "\n",
      "       [[-3.4039792e-02, -2.4015215e-01, -2.5847986e-02, ...,\n",
      "         -1.5648928e-01,  2.6566467e-01,  2.3303467e-01]]], dtype=float32)>, <tf.Tensor: shape=(20, 768), dtype=float32, numpy=\n",
      "array([[-0.50040084,  0.03004529,  0.64446235, ...,  0.21715291,\n",
      "        -0.16101325,  0.799442  ],\n",
      "       [-0.89464086, -0.14634508,  0.5497186 , ...,  0.28004083,\n",
      "        -0.5593555 ,  0.95087856],\n",
      "       [-0.52444184, -0.21960253, -0.7112523 , ..., -0.9153597 ,\n",
      "        -0.35748562,  0.54903835],\n",
      "       ...,\n",
      "       [-0.7134805 , -0.00454424, -0.68214947, ..., -0.83399755,\n",
      "        -0.24057381,  0.92036873],\n",
      "       [-0.56772   , -0.2166427 , -0.0317773 , ..., -0.40949616,\n",
      "        -0.3481277 ,  0.83307254],\n",
      "       [-0.50028753, -0.12147453,  0.76258427, ...,  0.57718426,\n",
      "        -0.35894257,  0.8149615 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "glue_batch = {key: val[:20] for key, val in glue_train.items()}\n",
    "print(bert_encoder(glue_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FKa9raYBSYw"
   },
   "source": [
    "### initializing a custom classifier\n",
    "\n",
    "We were asked to avoid using the default bert classifier, a manual classifier is instantiated using the nlp library, This helps us to play with various attributes like number of output classes, drop out rates and weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8q7E3X41MqX"
   },
   "outputs": [],
   "source": [
    "sentiment_classifier = nlp.modeling.models.BertClassifier(\n",
    "    bert_encoder,\n",
    "    num_classes = 2,\n",
    "    dropout_rate = transformer_config['dropout_rate'],\n",
    "    initializer=tf.keras.initializers.TruncatedNormal(\n",
    "          stddev=bert_config.initializer_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "lnLxDRTx2lMg",
    "outputId": "563c750e-e46f-48ab-b12a-f4018de1222a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14587045,  0.32071835],\n",
       "       [ 0.1134103 ,  0.45238513],\n",
       "       [-0.30765978,  0.15355203],\n",
       "       [ 0.14140067,  0.38017064],\n",
       "       [-0.31643736,  0.24233669],\n",
       "       [ 0.40680867,  0.13516405],\n",
       "       [ 0.30474538,  0.38137674],\n",
       "       [-0.08823901,  0.2683372 ],\n",
       "       [-0.02572563,  0.47956342],\n",
       "       [-0.07893646,  0.17836675],\n",
       "       [-0.13896203,  0.0202191 ],\n",
       "       [-0.47501814,  0.17931724],\n",
       "       [ 0.14061487,  0.4207766 ],\n",
       "       [ 0.0979514 ,  0.35018104],\n",
       "       [ 0.056511  , -0.04098345],\n",
       "       [-0.14823565,  0.1639602 ],\n",
       "       [ 0.05060878,  0.50964713],\n",
       "       [ 0.28556955,  0.63980305],\n",
       "       [-0.23541737,  0.29326594],\n",
       "       [ 0.53926706,  0.52923226]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_classifier(glue_batch,training=True).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BOFbKWcBwkV"
   },
   "source": [
    "### Setting up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4OG2Kz9nve9"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "train_data_size = len(glue_train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "sentiment_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Cqjwz2zB0_o"
   },
   "source": [
    "### Training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "ZiDoZPVUozwH",
    "outputId": "7ac25dd2-2fd5-4cfb-a438-bc6d28edf603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "268/268 [==============================] - 3636s 14s/step - loss: 0.5812 - accuracy: 0.7220 - val_loss: 0.4650 - val_accuracy: 0.7900\n",
      "Epoch 2/3\n",
      "268/268 [==============================] - 3667s 14s/step - loss: 0.4075 - accuracy: 0.8244 - val_loss: 0.5373 - val_accuracy: 0.7824\n",
      "Epoch 3/3\n",
      "268/268 [==============================] - 3641s 14s/step - loss: 0.2924 - accuracy: 0.8789 - val_loss: 0.5364 - val_accuracy: 0.8025\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_54c05401-11b9-46b8-a497-6378eb626476\", \"Cola_BERT.h5\", 1314519440)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_classifier.fit(\n",
    "      glue_train, glue_train_labels,\n",
    "      validation_data=(glue_validation, glue_validation_labels),\n",
    "      batch_size=32,\n",
    "      epochs=epochs)\n",
    "sentiment_classifier.save(\"Cola_BERT.h5\")\n",
    "files.download(\"Cola_BERT.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH1NzgAQB6TF"
   },
   "source": [
    "### Model Evaluation and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "MOopp5rezPB4",
    "outputId": "a147e954-4f0c-44e1-9a54-c94457635993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1]\n",
      "b'Us love they.' \t 0\n",
      "b'i am a man' \t 1\n",
      "b'It is nice to go abroad.' \t 1\n",
      "b'Mary came to be introduced by the bartender and I also came to be.' \t 0\n",
      "b'John often meets Mary.' \t 1\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "            b'Us love they.',\n",
    "            b'i am a man',\n",
    "            b'It is nice to go abroad.',\n",
    "            b'Mary came to be introduced by the bartender and I also came to be.',\n",
    "            b'John often meets Mary.'\n",
    "            ]\n",
    "test_sentence = preprocessing(\n",
    "    sst2_dict = {\n",
    "        'sentence': example_sentences,\n",
    "    },tokenizer = tokenizer)\n",
    "\n",
    "\n",
    "results = sentiment_classifier(test_sentence)\n",
    "result = tf.argmax(results,axis = 1).numpy()\n",
    "print(result)\n",
    "for i in range(len(example_sentences)):\n",
    "  print(\"{} \\t {}\".format(example_sentences[i],result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWp2vHA6CCBs"
   },
   "source": [
    "### Answers to the questions\n",
    "\n",
    "####**What is the tutorial classifying when using the GLUE MRPC data set?**\n",
    "\n",
    "GLUE/MRPC dataset contains pairs of sentences, The task is to build a model which compares the sentences and checks for the equivalence\n",
    "\n",
    "####**In addition to the input itself, the tutorial feeds two binary tensors for input mask and input type to the model.Is this necessary for the data set single sentence classification?**\n",
    "\n",
    "The bert encoder model demands 3 inputs they are \n",
    "\n",
    "* tokenized sentence tensor\n",
    "* input mask - This is necessary for all language classification models since this filters out the padded instances thus these padded instances are not considered while calculating the loss functions.\n",
    "\n",
    "\n",
    "####**How does the tokenization in BERT differ from the one in the previous Task 5?**\n",
    "\n",
    "In the previous task(NMT) we generated our own word tokens and we built the encoder model from scratch. In case of BERT the language model is already trained with a number of sentences and it has a vocabulory size of 30522 words, we just need to rip off this encoder part and attach a classifier with it to perform the required task.\n",
    "\n",
    "####**What is a [CLS] token and what is it used for?**\n",
    "\n",
    "bert model has a number of explicit tokens like [CLS],[SEP] and [MASK]. The [CLS] token is always appended at the beginning of the sentences. In the tensorflow example since the task has to deal with pairs of sentences we use a [SEP] token inbetween them. The BERT encoder model is pre trained with all these tokens in their respective places.\n",
    "\n",
    "####**Which part of the BERT encoding is used for the classification?**\n",
    "\n",
    "The input sentence is fed into the BERT encoder. These sentences are then encoded into a latent space representation. The encoded representation is then fed into a classifier model of our choice to perform the fine tuning.\n",
    "\n",
    "####**Does your answer match the output shape of the encoder?**\n",
    "\n",
    "The targeted output is a tensor of size (n,2) since the task at hand is a binary classification problem. The output shape of the encoder is (n,768) for the above encoder.\n",
    "\n",
    "####**Are the BERT encoder weights also fine-tuned to the task?**\n",
    "\n",
    "The encoder weights are also fine-tuned to the the task"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task-6 BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
